
# Module 4: Serverlessly query your data using Amazon Athena

Firstly, congratulations for reaching the last module in this tutorial. In this module you access and query the data that was extracted from your RDS MySql database and transformed to Parquet format.This data will be queiried directly from the your data lake without loading it into a database or data warehouse using Amazon Athena. 


### 3A: Access Data in Parquet format  

In this step, we will access the data we transformed to Parquet format and added to our data catalog as a table by our Glue Crawlers .

<details>
<summary><strong>Instructions for Accessing Glue table(expand for details)</strong></summary><p>

1. Go to [AWS Glue](https://console.aws.amazon.com/glue/home?region=us-east-1) on the console.

1. On the left pane, under the Data catlog section, select **Tables**.

1. Select the checkbox beside the table name and click the action dropdown button as **Job name** .

1. Select the an exisitng IAM role or Create a new one which has permission to your Amazon S3 sources, targets, temporary directory, scripts, and any libraries used by the job.

1. On the **This job runs** section, select **A proposed script generated by AWS Glue**

1. FOr **ETL language**, select ***Python***

1. You can choose to leave the remaining configurations on their default settings and click **Next**.

1. On the **Choose a data source** page, select the table extracted by your AWS Glue Crawler. Ensure that the classification is stated as ```mysql``` and click **Next**

1. On the **Choose a data target** page, select **Create tables in your data target** option

1. Under **Data store**, select **Amazon S3**. Under **Format**, select **Parquet**, Under **Target path**, select the S3 bucket you created in step 1 then click **Next**.

1.On the **Map the source columns to target columns** page, click **Next** and click **Save job and edit script** on the job Review section. 

1. The code and diagram for the job will be dispayed now as shown below

	<img src="images/transformJob.png" width="180%">

1. Click **Run job** and on the pop up menu, also click **Run job**. The job will begin running and it should take a few minutes to complete.

1. To check the status of the job, go the AWS Glue console and click **Jobs** select the job from the job list by checking the checkbox next to the job. A window with several tabs will appear beneath the job list. Select the **History** which will show you the **Run status** of the job. If the job is successful, the run status will be displayed as ```Succeeded``` . 
	
	See below for a sample screenshot
	
	<img src="images/jobStatus.png" width="180%">


</p></details>


### 3B: Create and run a new crawler to add the new table

Create and run a new crawler to discover and add the new Parquet table to your Data lake. The steps are similar to the steps in [Module 2](..2/) but ensure to choose S3 in the **Add a data store** page and select the S3 location of your transform data you defined in your AWS Glue transformation job. 

<img src="images/addDataStore.png" width="80%">
	
Run the crawler and check the for the new tables created.
 

### Next module


After you have verified that your AWS Glue crawler has successfully extracted data from your S3 bucket into your data lake, move onto the next module: [Serverlessly query your data using Amazon Athena](../4_QueryWithAthena)

